# ğŸ¤Ÿ Real-Time Sign Language Translator using MediaPipe & LSTM

This project is a real-time hand gesture recognition system that translates sign language alphabets (Aâ€“Z) into text using a webcam. It uses **MediaPipe** for hand landmark detection and an **LSTM-based neural network** for gesture classification.
---

## ğŸš€ Features

- Real-time gesture detection using webcam
- Translates Aâ€“Z (customizable gestures)
- Uses MediaPipe for accurate hand tracking
- Deep learning model with stacked LSTM layers
- Dynamic prediction smoothing and confidence filtering
- Simple, modular Python codebase

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ app.py               # Main real-time detection application
â”œâ”€â”€ train.py             # Model training script
â”œâ”€â”€ dataConversion.py    # Data preprocessing & landmark extraction
â”œâ”€â”€ function.py          # Core helper functions (landmark, drawing, extraction)
â”œâ”€â”€ model.h5             # Trained model weights
â”œâ”€â”€ model.json           # Trained model architecture
â”œâ”€â”€ MP_Data/             # Directory for processed training data
â””â”€â”€ README.md
```

---

## ğŸ§  Requirements

- Python 3.7+
- TensorFlow (2.x)
- OpenCV
- MediaPipe
- NumPy

ğŸ“¦ Install dependencies:

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install opencv-python mediapipe tensorflow numpy
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training the Model

The model was trained using:
- **Categorical Crossentropy** loss function
- **Adam optimizer** for adaptive learning
- **Train-test split**: 70% training, 15% validation, 15% testing
- Achieved **92% validation accuracy**

1. Collect or prepare images for each gesture (Aâ€“Z)
2. Extract hand keypoints with `dataConversion.py`
3. Train model with:

```bash
python train.py
```

This saves:
- `model.h5` â€“ trained model weights
- `model.json` â€“ model structure


---

## ğŸ¯ Running the Translator

Make sure `model.json` and `model.h5` are present in the project folder.

```bash
python app.py
```

Use your webcam to perform gestures. The recognized gesture will appear on the screen in real time.

### ğŸ“¸ Example Outputs

Below are some snapshots from the real-time detection window:

| Gesture | Detection Example |
|--------|--------------------|
| L      | ![Image](https://github.com/user-attachments/assets/c831aa13-95d5-4df0-91af-9ffbe79e8e84) |
| O      | ![Image](https://github.com/user-attachments/assets/6fb47a9e-3f50-4a3c-afb5-6c2724f24bcb)|
| W      | ![Image](https://github.com/user-attachments/assets/f42befaa-775d-4bd4-90ec-02982e44501a) |

> Replace the links above with your actual image URLs or GitHub-hosted images.


Make sure `model.json` and `model.h5` are present in the project folder.

```bash
python app.py
```

Use your webcam to perform gestures. The recognized gesture will appear on the screen in real time.

---

## âš™ï¸ Model Architecture

- Input: 70 frames of 21 landmarks (x, y, z) â†’ 63 features per frame
- 2 LSTM layers (64 â†’ 128units)
- 3 Dropout layers (0.3)
- 2 Dense layers (64 â†’ 32 units)
- Output: Softmax over number of gestures

---

## ğŸ”§ Customization

- âœ‹ Add your own gestures by editing `actions` in `function.py`
- ğŸ¥ Adjust `sequence_length` in `app.py` for responsiveness
- ğŸ” Tweak `threshold` to control confidence filtering
- ğŸ“Š Want accuracy graphs or evaluation? Add `model.evaluate()` or TensorBoard in `train.py`

---

## ğŸ™Œ Acknowledgments

- [MediaPipe](https://mediapipe.dev) for hand landmark detection
- [TensorFlow](https://tensorflow.org) for the LSTM model
- Inspired by projects like [Sign Language Detection](https://github.com/nicknochnack/Sign-Language-Detection)

---

## ğŸ“œ License

MIT License â€“ feel free to use, modify, and share!

---

## ğŸ‘‹ Get in Touch

If you found this project useful or want to collaborate, feel free to reach out or star the repo ğŸ’«

